# NLP的迁移学习-GPT篇

## 迁移学习和BERT

## GPT简介

## GPT设计思想

### few(zero) shot learning
![enter image description here](https://miro.medium.com/max/625/1*q-P5aQ7A6VlsfroP3ckg8A.jpeg)
### scale matters

### GTP vs BERT
-   GPT-2 and BERT at the two leading language models out there at time of writing in early 2020. They are the same in that they are both based on the transformer architecture, but they are fundamentally different in that BERT has just the  _encoder_  blocks from the transformer, whilst GPT-2 has just the  _decoder_  blocks from the transformer.

### GPT2/ GPT3
GPT-3 demonstrates that a language model trained on enough data can solve NLP tasks that it has never encountered. That is, GPT-3 studies the model as a general solution for many downstream jobs  **without fine-tuning**.
## look ahead



## reference
[gpt2 and bert](https://www.kaggle.com/residentmario/notes-on-gpt-2-and-bert-models)
[gpt3](https://medium.com/analytics-vidhya/openai-gpt-3-language-models-are-few-shot-learners-82531b3d3122)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE4NDI3NzEwMjYsMTk1NjQ3ODM5NywtMj
M0NDQ5MjQzLC04MzE5NDg3NzIsNzk3MTM2MTQ0LC0xNjAzMDY3
OTUxXX0=
-->