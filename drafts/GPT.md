# NLP的迁移学习-GPT篇

## 迁移学习和BERT

## GPT简介

## GPT设计思想

### few(zero) shot learning
![enter image description here](https://miro.medium.com/max/625/1*q-P5aQ7A6VlsfroP3ckg8A.jpeg)

![enter image description here](https://miro.medium.com/max/448/1*2dX-PZSNdmj0KOa-NmjrEA.jpeg)
### scale matters

### GTP vs BERT
-   GPT-2 and BERT at the two leading language models out there at time of writing in early 2020. They are the same in that they are both based on the transformer architecture, but they are fundamentally different in that BERT has just the  _encoder_  blocks from the transformer, whilst GPT-2 has just the  _decoder_  blocks from the transformer.

### GPT2/ GPT3
GPT-3 demonstrates that a language model trained on enough data can solve NLP tasks that it has never encountered. That is, GPT-3 studies the model as a general solution for many downstream jobs  **without fine-tuning**.
## look ahead



## reference
[gpt2 and bert](https://www.kaggle.com/residentmario/notes-on-gpt-2-and-bert-models)
[gpt3](https://medium.com/analytics-vidhya/openai-gpt-3-language-models-are-few-shot-learners-82531b3d3122)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTY2Mjg1NDUzOSwxOTU2NDc4Mzk3LC0yMz
Q0NDkyNDMsLTgzMTk0ODc3Miw3OTcxMzYxNDQsLTE2MDMwNjc5
NTFdfQ==
-->