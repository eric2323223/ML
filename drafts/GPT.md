# NLP的迁移学习-GPT篇

## 迁移学习和BERT

## GPT简介

## GPT设计思想

### few(zero) shot learning

### scale matters

### GTP vs BERT
-   GPT-2 and BERT at the two leading language models out there at time of writing in early 2020. They are the same in that they are both based on the transformer architecture, but they are fundamentally different in that BERT has just the  _encoder_  blocks from the transformer, whilst GPT-2 has just the  _decoder_  blocks from the transformer.

### GPT2/ GPT3

## look ahead



## reference
[gpt2 and bert](https://www.kaggle.com/residentmario/notes-on-gpt-2-and-bert-models)
[gpt3](https://medium.com/analytics-vidhya/openai-gpt-3-language-models-are-few-shot-learners-82531b3d3122)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTIzNDQ0OTI0MywtODMxOTQ4NzcyLDc5Nz
EzNjE0NCwtMTYwMzA2Nzk1MV19
-->