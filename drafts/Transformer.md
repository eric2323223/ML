# Transformer
## Attention
### multi-head attention VS convolution on 
## Vector similarity
## Positional encoding
- why not positional index?
## Residual connection
## point-wise FFN
## Mask
## 
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTExOTk2NDQyNDcsLTEzMTU5MTUwNSwxMj
E5MDIzMDIxXX0=
-->