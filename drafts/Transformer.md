# Transformer
## Theory and Model
### Encoder-Decoder architecture
### Attention
- one stone(attention), two birds(parallelize and long-range dependencies)
- 3 types of attention
- **multi-head attention** VS convolution on multiple channels
### Vector similarity
### Positional encoding
- why not positional index? extrapolate training samples
### Residual connection
### point-wise FFN
### Mask
## Training tricks
### layer normalization
### residual connection
### warn-up learning rate
### regularization
- dropout

## Resources
[Attention is all you need review]([https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html](https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html))
[The transformer - Attention is all you need]([https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XTEl6ugzZPY](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XTEl6ugzZPY))
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTgwNTM4ODI4NSwtMTExNDg0MTI5MiwyMT
I1NjQzNjUwLC0xNDYzMTUzNDM3LC0yMDA3MzUzNzQ1LC0yMjc1
NDExMjksLTEzMTU5MTUwNSwxMjE5MDIzMDIxXX0=
-->