# Transformer
## Attention
### multi-head attention VS convolution on multiple channels
## Vector similarity
## Positional encoding
- why not positional index?
## Residual connection
## point-wise FFN
## Mask
## 
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTIyNzU0MTEyOSwtMTMxNTkxNTA1LDEyMT
kwMjMwMjFdfQ==
-->