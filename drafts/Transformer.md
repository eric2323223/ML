# Transformer
## Model
### Attention
- one stone(attention), two birds(parallelize and long-range dependencies)
- multi-head attention VS convolution on multiple channels
### Vector similarity
### Positional encoding
- why not positional index?
### Residual connection
### point-wise FFN
### Mask
## Training tricks
### layer normalizaion
### residual connection
### warn-up learning rate
### regularization
- 
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTU1MTEwNjM0MywtMjI3NTQxMTI5LC0xMz
E1OTE1MDUsMTIxOTAyMzAyMV19
-->