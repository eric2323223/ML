# Transformer
## Attention
- one stone(attention), two birds(parallelize and long-range dependencies)
### multi-head attention VS convolution on multiple channels
## Vector similarity
## Positional encoding
- why not positional index?
## Residual connection
## point-wise FFN
## Mask
## 
<!--stackedit_data:
eyJoaXN0b3J5IjpbNzU5NTIwNzgwLC0yMjc1NDExMjksLTEzMT
U5MTUwNSwxMjE5MDIzMDIxXX0=
-->