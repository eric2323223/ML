# Transformer
## Attention

### multi-head attention VS convolution on multiple channels
## Vector similarity
## Positional encoding
- why not positional index?
## Residual connection
## point-wise FFN
## Mask
## 
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTY5NjkxOTI1OSwtMjI3NTQxMTI5LC0xMz
E1OTE1MDUsMTIxOTAyMzAyMV19
-->